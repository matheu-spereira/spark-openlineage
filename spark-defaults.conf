############################################
# Spark Master Configuration
############################################

# Define o endereço do nó master do Spark.
spark.master                           spark://spark-master:7077

############################################
# Event Logging e Spark History Server
############################################

# Habilita o log de eventos para que o History Server possa ler e exibir os jobs.
spark.eventLog.enabled                 true

# Caminho onde o Spark salvará os logs de eventos (no master e workers).
spark.eventLog.dir                     /opt/spark/events

# Caminho lido pelo History Server para exibir o histórico de aplicações Spark.
spark.history.fs.logDirectory          /opt/spark/events

# Intervalo de atualização do History Server para verificar novos logs.
spark.history.fs.update.interval	    30s
8
# Otimiza a leitura de aplicações em execução ainda não finalizadas.
spark.history.fs.inProgressOptimization.enabled true

# Caminho onde o estado do History Server é armazenado.
spark.history.store.path               /opt/spark/events

# Habilita a limpeza automática de logs antigos.
spark.history.fs.cleaner.enabled       true

# Define a idade máxima de logs antes de serem limpos (ex: "1d" = 1 dia).
spark.history.fs.cleaner.maxAge        1d

############################################
# JARs adicionais (conectores e extensões)
############################################

# Lista separada por vírgula dos JARs adicionais que o Spark deve carregar.
# Inclui conectores para Postgres, AWS S3, Delta Lake e OpenLineage.
spark.jars /opt/spark/jars/postgresql-42.6.0.jar,\
/opt/spark/jars/hadoop-aws-3.3.4.jar,\
/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,\
/opt/spark/jars/delta-spark_2.12-3.2.0.jar,\
/opt/spark/jars/delta-storage-3.2.0.jar,\
/opt/spark/jars/openlineage-spark_2.12-1.33.0.jar

############################################
# Configuração do acesso ao MinIO/S3 (via s3a)
############################################

# URL do endpoint do serviço S3 (MinIO, nesse caso).
spark.hadoop.fs.s3a.endpoint                 http://minio:9000

# Classe que implementa o sistema de arquivos S3A.
spark.hadoop.fs.s3a.impl                     org.apache.hadoop.fs.s3a.S3AFileSystem

# Chave de acesso (Access Key) para o MinIO.
spark.hadoop.fs.s3a.access.key               minio

# Chave secreta (Secret Key) para o MinIO.
spark.hadoop.fs.s3a.secret.key               minio123

# Usa "path-style access" (necessário para compatibilidade com MinIO).
spark.hadoop.fs.s3a.path.style.access        true



############################################
# Configurações de recursos do executor
############################################

# Número de núcleos de CPU que cada executor Spark utilizará.
# Controla o paralelismo e a alocação de recursos para tarefas em cada executor.
spark.executor.cores                      2

############################################
# Alocação dinâmica de executores
############################################

# Habilita a alocação dinâmica de executores, permitindo que o Spark ajuste automaticamente
# o número de executores baseando-se na carga atual do trabalho.
spark.dynamicAllocation.enabled           true

# Define o número mínimo de executores que o Spark manterá alocados quando a alocação dinâmica estiver ativada.
# Garante que pelo menos essa quantidade de executores esteja sempre disponível.
spark.dynamicAllocation.minExecutors      1

# Define o número máximo de executores que o Spark poderá alocar dinamicamente.
# Limita o uso máximo de recursos para evitar sobrecarga no cluster.
spark.dynamicAllocation.maxExecutors      2

spark.executor.instances                  1

############################################
# Configurações para Delta Lake
############################################

# Ativa a extensão Delta Lake no Spark SQL, permitindo usar comandos e funcionalidades Delta.
spark.sql.extensions                     io.delta.sql.DeltaSparkSessionExtension

# Configura o catálogo padrão do Spark para usar o catálogo Delta Lake,
# habilitando gerenciamento de tabelas Delta diretamente pelo Spark.
spark.sql.catalog.spark_catalog          org.apache.spark.sql.delta.catalog.DeltaCatalog
