# Base image with Python 3.12 and Debian Bullseye
FROM python:3.12-bullseye

# --------------------------------------------------
# Instala o Java 17 necessário para o Apache Spark
# --------------------------------------------------
RUN apt-get update && \
    apt-get install -y --no-install-recommends openjdk-17-jdk && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# --------------------------------------------------
# Define variáveis de ambiente essenciais
# --------------------------------------------------
ENV JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64"
ENV SPARK_HOME="/opt/spark"
ENV PATH="${JAVA_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"
ENV SPARK_MASTER_PORT="7077"
ENV SPARK_MASTER_HOST="spark-master"

# --------------------------------------------------
# Cria o diretório do Spark e define como diretório de trabalho
# --------------------------------------------------
RUN mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}

# --------------------------------------------------
# Baixa e instala o Apache Spark 3.5.6 com Hadoop 3
# --------------------------------------------------
RUN curl -fsSL https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz -o spark.tgz && \
    tar -xvzf spark.tgz --strip-components=1 && \
    rm -f spark.tgz

# --------------------------------------------------
# Copia configurações customizadas do Spark (se houver)
# --------------------------------------------------
COPY spark-defaults.conf "${SPARK_HOME}/conf/spark-defaults.conf"

# --------------------------------------------------
# Usa o Bash como shell padrão
# --------------------------------------------------
ENTRYPOINT ["/bin/bash"]
