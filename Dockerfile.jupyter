FROM python:3.12-bullseye

# --------------------------------------------------
# Instala Java 17 (requerido pelo Spark)
# --------------------------------------------------
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openjdk-17-jdk \
        curl \
        wget \
        git \
        ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# --------------------------------------------------
# Variáveis de ambiente
# --------------------------------------------------
ENV JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64"
ENV SPARK_HOME="/opt/spark"
ENV PATH="${JAVA_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"
ENV SPARK_MASTER_PORT="7077"
ENV SPARK_MASTER_HOST="spark-master"

# --------------------------------------------------
# Instala Apache Spark 3.5.6 + Hadoop 3
# --------------------------------------------------
RUN mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}

RUN curl -fsSL https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz -o spark.tgz && \
    tar -xvzf spark.tgz --strip-components=1 && \
    rm -f spark.tgz

# --------------------------------------------------
# Adiciona jars extras ao Spark (Postgres, AWS, Delta, OpenLineage)
# --------------------------------------------------
RUN mkdir -p ${SPARK_HOME}/jars && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/org/postgresql/postgresql/42.6.0/postgresql-42.6.0.jar && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.0/delta-spark_2.12-3.2.0.jar && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/io/delta/delta-storage/3.2.0/delta-storage-3.2.0.jar && \
    wget -q -P ${SPARK_HOME}/jars/ https://repo1.maven.org/maven2/io/openlineage/openlineage-spark_2.12/1.33.0/openlineage-spark_2.12-1.33.0.jar

# --------------------------------------------------
# Copia configurações do Spark
# --------------------------------------------------
COPY spark-defaults.conf ${SPARK_HOME}/conf/spark-defaults.conf

# --------------------------------------------------
# Instala pacotes Python essenciais para ciência de dados e integração com Spark
# --------------------------------------------------
RUN pip install --no-cache-dir \
    jupyterlab \
    pyspark \
    delta-spark==3.2.0

# --------------------------------------------------
# Diretório de trabalho para notebooks
# --------------------------------------------------
WORKDIR /home/jupyter

# --------------------------------------------------
# Executa JupyterLab no container
# --------------------------------------------------
CMD ["python3", "-m", "jupyterlab", "--ip", "0.0.0.0", "--port=8888", "--allow-root", "--NotebookApp.token=''", "--NotebookApp.password=''"]
